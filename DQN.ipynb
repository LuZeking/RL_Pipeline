{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import pprint\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from tianshou.data import Collector, PrioritizedVectorReplayBuffer, VectorReplayBuffer\n",
    "from tianshou.env import DummyVectorEnv\n",
    "from tianshou.policy import DQNPolicy\n",
    "from tianshou.trainer import offpolicy_trainer\n",
    "from tianshou.utils import TensorboardLogger\n",
    "from tianshou.utils.net.common import Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tianshou as ts\n",
    "\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--task', type=str, default='CartPole-v0')\n",
    "    parser.add_argument('--seed', type=int, default=1626)\n",
    "    parser.add_argument('--eps-test', type=float, default=0.05)\n",
    "    parser.add_argument('--eps-train', type=float, default=0.1)\n",
    "    parser.add_argument('--buffer-size', type=int, default=20000)\n",
    "    parser.add_argument('--lr', type=float, default=1e-3)\n",
    "    parser.add_argument('--gamma', type=float, default=0.9)\n",
    "    parser.add_argument('--n-step', type=int, default=3)\n",
    "    parser.add_argument('--target-update-freq', type=int, default=320)\n",
    "    parser.add_argument('--epoch', type=int, default=20)\n",
    "    parser.add_argument('--step-per-epoch', type=int, default=10000)\n",
    "    parser.add_argument('--step-per-collect', type=int, default=10)\n",
    "    parser.add_argument('--update-per-step', type=float, default=0.1)\n",
    "    parser.add_argument('--batch-size', type=int, default=64)\n",
    "    parser.add_argument(\n",
    "        '--hidden-sizes', type=int, nargs='*', default=[128, 128, 128, 128]\n",
    "    )\n",
    "    parser.add_argument('--training-num', type=int, default=10)\n",
    "    parser.add_argument('--test-num', type=int, default=100)\n",
    "    parser.add_argument('--logdir', type=str, default='log')\n",
    "    parser.add_argument('--render', type=float, default=0.)\n",
    "    parser.add_argument('--prioritized-replay', action=\"store_true\", default=False)\n",
    "    parser.add_argument('--alpha', type=float, default=0.6)\n",
    "    parser.add_argument('--beta', type=float, default=0.4)\n",
    "    parser.add_argument(\n",
    "        '--device', type=str, default='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    )\n",
    "    args = parser.parse_known_args()[0]\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_dqn(args=get_args()):\n",
    "    env = gym.make(args.task)\n",
    "    args.state_shape = env.observation_space.shape or env.observation_space.n\n",
    "    args.action_shape = env.action_space.shape or env.action_space.n\n",
    "    # train_envs = gym.make(args.task)\n",
    "    # you can also use tianshou.env.SubprocVectorEnv\n",
    "    train_envs = DummyVectorEnv(\n",
    "        [lambda: gym.make(args.task) for _ in range(args.training_num)]\n",
    "    )\n",
    "    # test_envs = gym.make(args.task)\n",
    "    test_envs = DummyVectorEnv(\n",
    "        [lambda: gym.make(args.task) for _ in range(args.test_num)]\n",
    "    )\n",
    "    # seed\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    train_envs.seed(args.seed)\n",
    "    test_envs.seed(args.seed)\n",
    "    # Q_param = V_param = {\"hidden_sizes\": [128]}\n",
    "    # model\n",
    "    net = Net(\n",
    "        args.state_shape,\n",
    "        args.action_shape,\n",
    "        hidden_sizes=args.hidden_sizes,\n",
    "        device=args.device,\n",
    "        # dueling=(Q_param, V_param),\n",
    "    ).to(args.device)\n",
    "    optim = torch.optim.Adam(net.parameters(), lr=args.lr)\n",
    "    policy = DQNPolicy(\n",
    "        net,\n",
    "        optim,\n",
    "        args.gamma,\n",
    "        args.n_step,\n",
    "        target_update_freq=args.target_update_freq,\n",
    "    )\n",
    "    # buffer\n",
    "    if args.prioritized_replay:\n",
    "        buf = PrioritizedVectorReplayBuffer(\n",
    "            args.buffer_size,\n",
    "            buffer_num=len(train_envs),\n",
    "            alpha=args.alpha,\n",
    "            beta=args.beta,\n",
    "        )\n",
    "    else:\n",
    "        buf = VectorReplayBuffer(args.buffer_size, buffer_num=len(train_envs))\n",
    "    # collector\n",
    "    train_collector = Collector(policy, train_envs, buf, exploration_noise=True)\n",
    "    test_collector = Collector(policy, test_envs, exploration_noise=True)\n",
    "    # policy.set_eps(1)\n",
    "    train_collector.collect(n_step=args.batch_size * args.training_num)\n",
    "    # log\n",
    "    log_path = os.path.join(args.logdir, args.task, 'dqn')\n",
    "    writer = SummaryWriter(log_path)\n",
    "    logger = TensorboardLogger(writer)\n",
    "\n",
    "    def save_fn(policy):\n",
    "        torch.save(policy.state_dict(), os.path.join(log_path, 'policy.pth'))\n",
    "\n",
    "    def stop_fn(mean_rewards):\n",
    "        return mean_rewards >= env.spec.reward_threshold\n",
    "\n",
    "    def train_fn(epoch, env_step):\n",
    "        # eps annnealing, just a demo\n",
    "        if env_step <= 10000:\n",
    "            policy.set_eps(args.eps_train)\n",
    "        elif env_step <= 50000:\n",
    "            eps = args.eps_train - (env_step - 10000) / \\\n",
    "                40000 * (0.9 * args.eps_train)\n",
    "            policy.set_eps(eps)\n",
    "        else:\n",
    "            policy.set_eps(0.1 * args.eps_train)\n",
    "\n",
    "    def test_fn(epoch, env_step):\n",
    "        policy.set_eps(args.eps_test)\n",
    "\n",
    "    # trainer\n",
    "    result = offpolicy_trainer(\n",
    "        policy,\n",
    "        train_collector,\n",
    "        test_collector,\n",
    "        args.epoch,\n",
    "        args.step_per_epoch,\n",
    "        args.step_per_collect,\n",
    "        args.test_num,\n",
    "        args.batch_size,\n",
    "        update_per_step=args.update_per_step,\n",
    "        train_fn=train_fn,\n",
    "        test_fn=test_fn,\n",
    "        stop_fn=stop_fn,\n",
    "        save_fn=save_fn,\n",
    "        logger=logger,\n",
    "    )\n",
    "    assert stop_fn(result['best_reward'])\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "        pprint.pprint(result)\n",
    "        # Let's watch its performance!\n",
    "        env = gym.make(args.task)\n",
    "        policy.eval()\n",
    "        policy.set_eps(args.eps_test)\n",
    "        collector = Collector(policy, env)\n",
    "        result = collector.collect(n_episode=1, render=args.render)\n",
    "        rews, lens = result[\"rews\"], result[\"lens\"]\n",
    "        print(f\"Final reward: {rews.mean()}, length: {lens.mean()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pdqn(args=get_args()):\n",
    "    args.prioritized_replay = True\n",
    "    args.gamma = .95\n",
    "    args.seed = 1\n",
    "    test_dqn(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1:  95%|#########5| 9530/10000 [00:14<00:00, 678.88it/s, env_step=9530, len=200, n/ep=1, n/st=10, rew=200.00]             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'best_result': '199.74 ± 1.87',\n",
      " 'best_reward': 199.74,\n",
      " 'duration': '14.15s',\n",
      " 'test_episode': 600,\n",
      " 'test_speed': '18801.27 step/s',\n",
      " 'test_step': 85235,\n",
      " 'test_time': '4.53s',\n",
      " 'train_episode': 493,\n",
      " 'train_speed': '990.82 step/s',\n",
      " 'train_step': 9530,\n",
      " 'train_time/collector': '1.98s',\n",
      " 'train_time/model': '7.63s'}\n",
      "Final reward: 200.0, length: 200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\tianshou\\data\\collector.py:61: UserWarning: Single environment detected, wrap to DummyVectorEnv.\n",
      "  warnings.warn(\"Single environment detected, wrap to DummyVectorEnv.\")\n"
     ]
    }
   ],
   "source": [
    "test_dqn(get_args())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1: 10001it [00:08, 1149.01it/s, env_step=10000, len=200, loss=0.320, n/ep=0, n/st=10, rew=200.00]                          \n",
      "Epoch #2:   3%|2         | 270/10000 [00:00<00:06, 1477.77it/s, env_step=10270, len=153, loss=0.379, n/ep=0, n/st=10, rew=153.00]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1: test_reward: 193.060000 ± 12.457785, best_reward: 193.060000 ± 12.457785 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #2:  76%|#######6  | 7630/10000 [00:07<00:02, 1041.37it/s, env_step=17630, len=200, n/ep=1, n/st=10, rew=200.00]            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'best_result': '197.46 ± 6.05',\n",
      " 'best_reward': 197.46,\n",
      " 'duration': '16.89s',\n",
      " 'test_episode': 600,\n",
      " 'test_speed': '22325.26 step/s',\n",
      " 'test_step': 82848,\n",
      " 'test_time': '3.71s',\n",
      " 'train_episode': 433,\n",
      " 'train_speed': '1337.39 step/s',\n",
      " 'train_step': 17630,\n",
      " 'train_time/collector': '2.87s',\n",
      " 'train_time/model': '10.31s'}\n",
      "Final reward: 200.0, length: 200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\tianshou\\data\\collector.py:61: UserWarning: Single environment detected, wrap to DummyVectorEnv.\n",
      "  warnings.warn(\"Single environment detected, wrap to DummyVectorEnv.\")\n"
     ]
    }
   ],
   "source": [
    "# policy.load_state_dict(torch.load('log/dqn.pth'))\n",
    "test_pdqn(args=get_args())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1:  95%|#########5| 9530/10000 [00:10<00:00, 936.59it/s, env_step=9530, len=200, n/ep=1, n/st=10, rew=200.00]             \n"
     ]
    }
   ],
   "source": [
    "args = get_args()\n",
    "env = gym.make(args.task)\n",
    "args.state_shape = env.observation_space.shape or env.observation_space.n\n",
    "args.action_shape = env.action_space.shape or env.action_space.n\n",
    "# train_envs = gym.make(args.task)\n",
    "# you can also use tianshou.env.SubprocVectorEnv\n",
    "train_envs = DummyVectorEnv(\n",
    "    [lambda: gym.make(args.task) for _ in range(args.training_num)]\n",
    ")\n",
    "# test_envs = gym.make(args.task)\n",
    "test_envs = DummyVectorEnv(\n",
    "    [lambda: gym.make(args.task) for _ in range(args.test_num)]\n",
    ")\n",
    "# seed\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "train_envs.seed(args.seed)\n",
    "test_envs.seed(args.seed)\n",
    "# Q_param = V_param = {\"hidden_sizes\": [128]}\n",
    "# model\n",
    "net = Net(\n",
    "    args.state_shape,\n",
    "    args.action_shape,\n",
    "    hidden_sizes=args.hidden_sizes,\n",
    "    device=args.device,\n",
    "    # dueling=(Q_param, V_param),\n",
    ").to(args.device)\n",
    "optim = torch.optim.Adam(net.parameters(), lr=args.lr)\n",
    "policy = DQNPolicy(\n",
    "    net,\n",
    "    optim,\n",
    "    args.gamma,\n",
    "    args.n_step,\n",
    "    target_update_freq=args.target_update_freq,\n",
    ")\n",
    "# buffer\n",
    "if args.prioritized_replay:\n",
    "    buf = PrioritizedVectorReplayBuffer(\n",
    "        args.buffer_size,\n",
    "        buffer_num=len(train_envs),\n",
    "        alpha=args.alpha,\n",
    "        beta=args.beta,\n",
    "    )\n",
    "else:\n",
    "    buf = VectorReplayBuffer(args.buffer_size, buffer_num=len(train_envs))\n",
    "# collector\n",
    "train_collector = Collector(policy, train_envs, buf, exploration_noise=True)\n",
    "test_collector = Collector(policy, test_envs, exploration_noise=True)\n",
    "# policy.set_eps(1)\n",
    "train_collector.collect(n_step=args.batch_size * args.training_num)\n",
    "# log\n",
    "log_path = os.path.join(args.logdir, args.task, 'dqn')\n",
    "writer = SummaryWriter(log_path)\n",
    "logger = TensorboardLogger(writer)\n",
    "\n",
    "def save_fn(policy):\n",
    "    torch.save(policy.state_dict(), os.path.join(log_path, 'policy.pth'))\n",
    "\n",
    "def stop_fn(mean_rewards):\n",
    "    return mean_rewards >= env.spec.reward_threshold\n",
    "\n",
    "def train_fn(epoch, env_step):\n",
    "    # eps annnealing, just a demo\n",
    "    if env_step <= 10000:\n",
    "        policy.set_eps(args.eps_train)\n",
    "    elif env_step <= 50000:\n",
    "        eps = args.eps_train - (env_step - 10000) / \\\n",
    "            40000 * (0.9 * args.eps_train)\n",
    "        policy.set_eps(eps)\n",
    "    else:\n",
    "        policy.set_eps(0.1 * args.eps_train)\n",
    "\n",
    "def test_fn(epoch, env_step):\n",
    "    policy.set_eps(args.eps_test)\n",
    "\n",
    "# trainer\n",
    "result = offpolicy_trainer(\n",
    "    policy,\n",
    "    train_collector,\n",
    "    test_collector,\n",
    "    args.epoch,\n",
    "    args.step_per_epoch,\n",
    "    args.step_per_collect,\n",
    "    args.test_num,\n",
    "    args.batch_size,\n",
    "    update_per_step=args.update_per_step,\n",
    "    train_fn=train_fn,\n",
    "    test_fn=test_fn,\n",
    "    stop_fn=stop_fn,\n",
    "    save_fn=save_fn,\n",
    "    logger=logger,\n",
    ")\n",
    "assert stop_fn(result['best_reward'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'best_result': '199.74 ± 1.87',\n",
      " 'best_reward': 199.74,\n",
      " 'duration': '10.22s',\n",
      " 'test_episode': 600,\n",
      " 'test_speed': '24355.80 step/s',\n",
      " 'test_step': 85235,\n",
      " 'test_time': '3.50s',\n",
      " 'train_episode': 493,\n",
      " 'train_speed': '1417.27 step/s',\n",
      " 'train_step': 9530,\n",
      " 'train_time/collector': '1.49s',\n",
      " 'train_time/model': '5.24s'}\n",
      "Final reward: 200.0, length: 200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\tianshou\\data\\collector.py:61: UserWarning: Single environment detected, wrap to DummyVectorEnv.\n",
      "  warnings.warn(\"Single environment detected, wrap to DummyVectorEnv.\")\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(result)\n",
    "# Let's watch its performance!\n",
    "env = gym.make(args.task)\n",
    "policy.eval()\n",
    "policy.set_eps(args.eps_test)\n",
    "collector = Collector(policy, env)\n",
    "result = collector.collect(n_episode=1, render=args.render)\n",
    "rews, lens = result[\"rews\"], result[\"lens\"]\n",
    "print(f\"Final reward: {rews.mean()}, length: {lens.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy.load_state_dict(torch.load('dqn.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\tianshou\\data\\collector.py:61: UserWarning: Single environment detected, wrap to DummyVectorEnv.\n",
      "  warnings.warn(\"Single environment detected, wrap to DummyVectorEnv.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n/ep': 1,\n",
       " 'n/st': 185,\n",
       " 'rews': array([185.]),\n",
       " 'lens': array([185]),\n",
       " 'idxs': array([0]),\n",
       " 'rew': 185.0,\n",
       " 'len': 185.0,\n",
       " 'rew_std': 0.0,\n",
       " 'len_std': 0.0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy.eval()\n",
    "policy.set_eps(0.05)\n",
    "collector = ts.data.Collector(policy, env, exploration_noise=True)\n",
    "collector.collect(n_episode=1, render=1 / 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c5e48f84046969b800ff52f6d80523bcd1ca3fb1a99f1449e4197bf6c73dc096"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
